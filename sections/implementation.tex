\documentclass[../index.tex]{subfiles}

\begin{document}

In this chapter and the following, we will cover in greater detail the
changes and extensions to 3DF, that we implemented over the course of
this work. This chapter covers logging and other instrumentation that
was added to perform the experiments, the move from eager to deferred
synthesis, and support for multi-tenant dataflows, whereas the next
chapter is dedicated to the implementation of delta queries and our
worst-case optimal dataflow join operator.

\subsection{Logging and Instrumentation} \label{logging}

A fair bit of instrumentation was necessary in order to monitor the
dataflow graph as it evolves over time, the number of tuples held in
arrangements, and many other metrics. We built dataflow sources
converting Timely's internal logging streams into higher-level log
attributes and analyzed them interactively at run-time, using 3DF
itself.

Timely Dataflow provides an extensible set of internal dataflow
streams, onto which internal events are published. Logged events
include among others the creation, shutdown, and scheduling of
operators, activity on communication channels, and progress
tracking. Differential Dataflow uses the same subsystem to provide
additional information about the creation and merging of trace
batches. These are sufficient to infer, for any relevant time, the
number of arranged tuples maintained at each arrangement in the
dataflow.

3DF attributes can be fed by arbitrary data sources. This is
facilitated by \emph{source} operators, which read from external data
sources and convert inputs to one or more Timely streams. We created
two new 3DF sources, \texttt{TimelyLogging} and
\texttt{DifferentialLogging}.

The \texttt{TimelyLogging} source allows clients to source only the
subset of logging attributes that they are interested in. It also
performs some pre-processing on the raw logging streams, converting
hierarchical scope addresses into the corresponding edges of the
dataflow graph.

\begin{lstlisting}[language=json][h!]
{:TimelyLogging
  {:attributes [:timely.event.operates/name
                :timely.event.operates/shutdown?]}}
\end{lstlisting}

Similarly, the \texttt{DifferentialLogging} source will not expose
batching and merging events directly, but rather derive from them
changes to the number of tuples held by each arrangement. Batch events
imply an increase in the number of tuples held (by the size of the
batch), merge events imply a decrease (the difference between the
compacted size of the merge result and the sum of the size of the
input batches).

Taken together, this makes it easy to write queries such as the
following, asking for the total number of tuples arranged at operators
that haven't been shut down.

\begin{lstlisting}[language=datalog, style=colorlog]
;; total number of tuples arranged at alive operators

[:find ?name (sum ?size)
 :where
 [?x :differential.event/size ?size]
 [?x :timely.event.operates/name ?name]  
 (not [?x :timely.event.operates/shutdown? true])]
\end{lstlisting}

For making accurate measurements it also became necessary to side-step
3DFs default behaviour of forwarding results directly back to clients,
as doing so would introduce serialization and network latency. We
therefore introduced a new type of plan stages called \emph{sinks}, as
well as a measurement sink implementation. This sink will swallow all
its inputs and keep track of the computational frontier. Whenever the
frontier advances (thus indicating the completion of a round of
inputs), it will append the number of milliseconds since the close of
the previous epoch to a logfile. It will also report whenever the
frontier becomes empty, indicating the completion of the entire
computation.

We then extended the 3DF API in order to allow clients to specify an
optional sink configuration that, when provided, will cause 3DF to
forgo forwarding results via WebSocket and instead process them with
the specified sink. Additionally, we extended the API to allow for an
optional granularity (in seconds), indicating the maximum rate at
which a client desires outputs. A granularity of one, for example,
would cause outputs to be delayed and consolidated up to the next full
second. This made it easy to monitor e.g. tuple counts on the live
dataflow. Finally, we added an option to disable logging on queries,
such as not to run into infinte loops when logging queries that
consume the logging relations themselves.

\subsection{Lazy Synthesis} \label{lazy-synthesis}

In \autoref{case-eagerness} we have seen that demand-driven synthesis
of query plans is a pre-requisite to avoid needlessly computing rules
which might materialize large numbers of tuples, but which in practice
would never be used without other constraints of higher selectivity
present. Eager synthesis of queries limits the available plan space
drastically. We changed 3DFs implementation strategy from eager to
lazy, introducing a mechanism to track plan dependencies in the
process.

First, we split the existing query registration API into a two-step
process: \emph{registration} and \emph{interest}. We extended 3DF
worker state with a rule store. Upon receiving a registration request,
workers now merely store the provided plan (assuming it doesn't clash
with a known rule of the same name). A rule is only synthesised once
the first interest request depending on it is received.

Therefore, we added a mechanism to gather plan dependencies. Queries
can depend on attributes and on other rules, that were registered in
previous requests (maybe by other users). Like all other aspects of
synthesis, dependencies are collected recursively, starting at the
leafs of a query plan. The resulting set of rules requiring synthesis
is brought into a canonical order, to ensure that the same exact
dataflow graph is created on all workers. This is necessary, as Timely
creates operator and channel identifiers based on their creation
order. Redundant dependencies are removed.

At this point a decision can be made for each named dependency, for
which an existing arrangement is available, on whether to re-use the
arrangement or resolve the dependency into its bindings and
re-synthesize them from scratch. For the reasons explored in chapter
\ref{case-redundant-dataflows}, the default policy is to always
synthesize from scratch, re-using only attributes indices.

With these changes, clients can define complex queries interactively,
across many different rules, without bringing the system to a
halt. Storing plan representations has other benefits, which we might
explore in future work. In particular, such a representation could be
used to dynamically query the implied graph of rule dependencies and
correlate it with the synthesized dataflow graph, whose structure can
be derived from the logging stream, as explained in the previous
section.

\subsection{Multi-tenant Routing}

In \autoref{case-redundant-dataflows} we saw that multi-tenant
dataflows, where applicable, allow us to serve many clients without
creating additional dataflow elements, and without limiting the
optimization potential of the overall dataflow. We added a new routing
strategy, in order to support such multi-tenant dataflows in 3DF. Our
current implementation is not automated, in that it requires clients
to indicate which dataflows they would like to share.

In order to be dynamically parameterizable at runtime, a query's fixed
parameters must first be replaced by joins on special parameter
relations. E.g.

\begin{lstlisting}[language=datalog, style=colorlog]
;; static
[:find ?speed
 :where [<x> :device/speed ?speed]]  

;; dynamic
[:find ?device ?speed
 :where
 [?device :device/speed ?speed]
 [_ :param/device ?device]]
\end{lstlisting}

We achieve multi-tenancy by tagging inputs on parameter relations with
a token, uniquely identifying a tenant. Client tokens are also
included as part of the output projection. Tenants register their
interest, indicating the offset at which tenant tokens are to be found
within the output tuples.

\begin{lstlisting}[language=datalog, style=colorlog]
[:find ?tenant ?device ?speed
 :where
 [?device :device/speed ?speed]
 [?tenant :param/device ?device]]
\end{lstlisting}

What remains is the correct routing of results back to clients,
according to the tenant token in each output tuple. This way, a given
tenant only sees the results that are of interest to them. Our current
implementation re-uses the connection tokens assigned by the WebSocket
server. Client tokens are unsigned 64-bit integers.

Timely Dataflow makes use of \emph{exchange pacts} to define how data
is routed between operators. For single-tenant dataflows, all workers
therefore must be aware of which of them maintains the client
connection. For a given request, we call this worker the
\emph{owner}. Dataflow results are then exchanged using a static
policy (w.r.t to the dataflow's data plane), which routes every output
tuple to the owner. Each worker additionally maintains a mapping of
query names to the client tokens, which have expressed interest in
receiving results from that query. As long as all relevant results are
exchanged correctly to the owning worker, all interested clients will
therefore receive copies.

Knowledge about the owner is broadcasted during \emph{sequencing}, a
synchronizing dataflow into which all workers input all requests
received from clients. Sequencing ensures, that all workers process
the sequence of commands in the exact same order. Workers tag requests
that they have received from a client with their own worker id, before
broadcasting them via the sequencing dataflow.

In the multi-tenant setting, the routing policy is no longer static,
but rather data dependent. Workers must therefore maintain a mapping
of tenants to their owning workers. New entries into this mapping are
created on interest requests, and removed whenever clients disconnect
or issue a uninterest request. Owning workers broadcast notifiations,
whenever any of their clients disconnects, s.t. all other workers may
clean up their mappings.

\end{document}
