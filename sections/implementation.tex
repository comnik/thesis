\documentclass[../index.tex]{subfiles}

\begin{document}

\subsection{Indexing}

In order to make use of the worst-case optimal dataflow-join framework
described in \texttt{@TODO} we could no longer maintain attributes as just
an arranged collection of `(e, v)` pairs. The implementation of the
`PrefixExtender` trait for attributes requires two additional traces,
one to keep track of the number of extensions the attribute would
propose for a given prefix and a trace indexed in a way suitable to
efficiently validate proposals made by other extenders.

Additionally, attributes might be placed at a stage in the delta query
pipeline, at which prefixes already bind the value symbol. In that
case, reversed versions of all three of the above arrangements must be
at hand.

We therefore introduced a `CollectionIndex` structure which holds all
three arrangements for a given direction and a given attribute. 3DF
workers maintain separate mappings from attribute names to their
forward and reverse collection indices. This simplifies the types and
ownership involved.

Although not client-configurable at the time of this writing, it makes
sense to skip the reverse indices for certain attributes. An example
would be one-way mappings used for string interning, or unary
attributes indicating categorical features.

\subsection{Lazy Synthesis}

In the \texttt{Eager Synthesis} scenario we have seen that demand-driven
synthesis of query plans is a pre-requisite to needlessly
materializing rules, which materialize large numbers of tuples, but
would never even be used without other constraints of higher
selectivity.

As explained in \texttt{Background/3DF}, 3DF used to synthesize all rules
eagerly. Over the course of this work, we extended 3DF workers with a
rule store. Upon receiving a `Register` command, workers now merely
store the provided plan (assuming it doesn't clash with a known rule
of the same name). Synthesis of a rule `q` will only happen, once the
first `Interest` command for `q` is received.

A few more changes were required. In particular, a mechanism to gather
query dependencies had to be added. Queries can depend on attributes
and on other rules (that were registered in advance). This captured by
a simple struct:

``` clojure
\emph{/} Description of everything a plan needs prior to synthesis.
pub struct Dependencies \{
    \emph{/} NameExpr's used by this plan.
    pub names: HashSet<String>,
    \emph{/} Attributes queries in Match* expressions.
    pub attributes: HashSet<Aid>,
\}
```

Like all other aspects of synthesis, `Dependency` structs are
collected recursively starting at the leafs of the query plan (data
patterns such as `MatchA` and references via `NameExpr`). The
resulting set of rules requiring synthesis is brought into a canonical
order, to ensure that the same exact dataflow graph is created on all
workers.

@TODO Talk about deciding between depending on NameExpr by name or resolving into dependencies?
@TODO Talk about resulting unification of NameExpr and RuleExpr

\subsection{Logging and Instrumentation}

A fair bit of instrumentation is necessary in order to monitor the
dataflow graph as it evolves over time, the number of tuples held in
arrangements, and many other metrics.

To that end, Timely Dataflow provides an extensible set of internal
dataflow streams, onto which internal events are published. Logged
events include among others the creation, shutdown, and scheduling of
operators, activity on communication channels, and progress tracking.

Differential Dataflow uses the same subsystem to provide additional
information about the creation and merging of trace batches. These are
sufficient to infer, for any relevant time, the number of arranged
tuples maintained at each arrangement in the dataflow.

Deriving relevant signals from the stream of logging events
dynamically via 3DF's reactive query engine seemed like a natural
fit. We created two new 3DF sources, `TimelyLogging` and
`DifferentialLogging`.

The `TimelyLogging` source allows clients to source only the subset of
logging attributes that they are interested in (see \texttt{LISTING
BELOW}). It also performs some pre-processing on the raw logging
streams, converting hierarchical scope addresses into the
corresponding edges of the dataflow graph.

``` clojure
(register-source
  \{:TimelyLogging
   \{:attributes [:timely.event.operates/name
                 :timely.event.operates/shutdown?]\}\})
```

Similarly, the `DifferentialLogging` source will not expose batching
and merging events directly, but rather derive from them changes to
the number of tuples held by each arrangement. Batch events imply an
increase in the number of tuples held (by the size of the batch),
merge events imply a decrease (the difference between the compacted
size of the merge result and the sum of the size of the input
batches).

Taken together, this makes it easy to write queries such as the
following, asking for the total number of tuples arranged at operators
that haven't been shut down.

``` clojure
;; total number of tuples arranged at alive operators

[:find (sum ?size)
 :where
 [?x :differential.event/size ?size]
 (not [?x :timely.event.operates/shutdown? true])]
```

\subsection{Bindings} \label{bindings}

Over the course of this work it became evident that a few extensions
to the query plan representation would be necessary to (a) better
utilize the new primitive offered by worst-case optimal, n-way join
processing and (b) canonicalize plan representation in order to expose
more opportunities for sharing dataflows between multiple clients, as
described in \texttt{@TODO}.

From the discussion in \cite{veldhuizen2012leapfrog} we learn that the
Leapfrog Triejoin worst-case optimal join algorithm can be utilized to
implement many common features of relational query engines:
conjunctions (joins), disjunctions (unions), negations (antijoins),
and filtering by various predicates. This is achieved by additional
implementations of the core trie iterator interface. Similar
extensions exist within the worst-case optimal dataflow-join framework
due to \cite{ammar2018distributed}, which we have integrated into 3DF
as part of this work.

[@TODO Recall] that any participant in a worst-case optimal dataflow
join must implement the `PrefixExtender` trait described in
\texttt{@TODO}.

For the purposes of the query language, we will refer to such
implementations as \emph{bindings}, for their property of binding
possible values to variables. We've provided \texttt{PrefixExtender}
implementations for attribute bindings, constants, binary predicates,
and negation.

Some bindings, such as for predicates and negation, are exclusively
constraining (shrinking the space of possible values for a given
variable), others, such as attributes and constants, can also provide
values.

Converting between the existing clause language described in \ref{3df}
and this new language of bindings is straightforward. The basic data
pattern \texttt{[?e :a ?v]} is expressed as \texttt{MatchA(?e, :a,
  ?v)} and as \texttt{attribute(?e, :a, ?v)}
respectively. \texttt{MatchEA} and \texttt{MatchAV} can be broken down
into a \texttt{attribute(?e, :a, ?v)} combined with a
\texttt{constant(?e, x)} or \texttt{constant(?v, x)} respectively.

The existing two-way join operator \texttt{Join(left, right)} is
resolved as the union of resolving both child plans into their
bindings.

@TODO antijoin

\texttt{Filter} plan stages map directly onto the corresponding
\texttt{binary\_predicate} binding.

Projections, aggregations, disjunctions, and functional transforms
remain unchanged.

Splitting off constant bindings, transforms, aggregations, and
projections from the heavy-lifting required to resolve conjunctions
has the added benefit of exposing additional opportunities for sharing
dataflows between clients. Consider the following two queries.

\begin{verbatim}
[:find ?person
 :where
 (rule ?person)
 [?person :person/name "Alice"]]

[:find ?person
 :where
 (rule ?person)
 [?person :person/name "Bob"]]
\end{verbatim}

We've described the various trade-offs to factor-in when considering
to share the evaluation of `rule` between the two clients in chapter
\texttt{@TODO}. Here we merely want to call attention to the resulting
representation in the language of bindings.

\begin{verbatim}
[into_bindings("rule"),
 attribute(?person, :person/name, ?name),
 constant(?name, "Alice")]

[into_bindings("rule"),
 attribute(?person, :person/name, ?name),
 constant(?name, "Bob")]
\end{verbatim}

\subsection{A Worst-Case Optimal N-Way Join Operator}

In order to be able to express the worst-case optimal join strategy
explored throughout this work, a new query plan stage and
corresponding operator implementation had to be added. We've based the
present implementation off of \cite{dogsdogsdogs} and
\cite{dataflowjoin}. The resulting operator is called \texttt{Hector}
and will be referred to as such throughout this chapter.

\texttt{@TODO GenericJoin lineage}

At a high-level, the Hector operator provides the following
capability: Given a set of bindings (as described in \ref{bindings})
and a set of target variables, find all possible variable assignments
that satisfy all bindings. The following Hector plan stage would
therefore be sufficient to express a simple triangle query:

\begin{verbatim}
{Hector
 {variables: [?a, ?b, ?c],
  bindings:  [attribute(?a, :edge, ?b),
              attribute(?b, :edge, ?c),
              attribute(?a, :edge, ?c),]}}
\end{verbatim}

If no bindings are passed, Hector will throw an error. If a single
binding is passed (which must be an `attribute` binding, as no other
binding can provide tuples), Hector will merely perform a projection
onto the target variables.

\texttt{@TODO n=2 case}

For the general-arity case, Hector employs the delta-query technique
explained in \texttt{@TODO}. This means that a separate dataflow will
be constructed for each tuple-providing binding that may experience
change. Only `attribute` bindings can provide tuples in our current
implementation. By default, all attributes are assumed to experience
change. Continuing with our example, Hector would therefore create
three dataflows. For each of them, we will refer to the generating
binding as the \emph{source binding} and to the corresponding dataflow
as the \emph{delta pipeline}.

\begin{verbatim}
(1) d_edge(a, b) -> edge(b, c) -> edge(a, c)
(2) d_edge(b, c) -> edge(a, b) -> edge(a, c)
(3) d_edge(a, c) -> edge(a, b) -> edge(b, c)
\end{verbatim}

All delta pipelines are executed concurrently at the dataflow
level. This can lead to inconsistencies as we might derive the same
output change on multiple pipelines, when sources change
concurrently. Assume for example we have a graph containing the
triangle $[100,200,300]$, formed by the edges $(100,200)$,
$(200,300)$, and $(100,300)$. A single change $((100,200),-1)$ to the
\texttt{:edge} attribute will cause all three pipelines to derive the
same retraction $([100,200,300],-1)$.

To prevent this, we must impose a logical order on the computation. In
particular, we must ensure that the retraction of \texttt{@TODO}

Delta pipelines are therefore created within a new, nested scope
carrying the `AltNeu` timestamp type. Upon use, attributes such as
\texttt{:edge} are imported and wrapped with the corresponding
\texttt{AltNeu} timestamp. We cache imported arrangements by attribute
name, to prevent redundant imports.

Recall that a worst-case optimal join picks an appropriate variable
order (more on this crucial step later), along which a collection of
prefixes (initially containing only the empty prefix) is extended to
bind more and more symbols. While conceptually this is correct, it
does not translate directly into the dataflow setting. Dataflows must
always start with some source of input. In our case, the finest
grained source of input available are tuple-providing bindings,
i.e. the \texttt{attribute} binding — which already binds two symbols!

In order to consider changes to each individual variable separately,
we could break attributes further down into unified input collections
for each of their constituent variables. But this would have to be
done in a separate dataflow for each combination of bindings. Instead,
we would like to start with attribute inputs, and thus with prefixes
of length two.

In order to get away with this we must make sure to handle conflicts
on the variables of each source binding. Consider a Hector plan stage
involving (possibly amongst others) both an \texttt{attribute(?user,
  :user/name, ?name)} and a \texttt{constant(?name, "Alice")}
binding. When creating the delta pipeline starting from the
\texttt{:user/name} attribute, we would never give the constant
binding a chance to narrow down the collection of all usernames to
just those equal to "Alice".

It is straightforward to detect such conflicts for a given source
binding, as we can look for any of the remaining bindings for which
all of their variables are already bound by the prefix. In our
example, the constant binding binds only \texttt{?name}, which is
already bound by the prefix \texttt{[?user ?name]}.

The same can happen for attribute bindings. Consider
\texttt{attribute(?a, :edge, ?b)} and \texttt{attribute(?b, :edge,
  ?a)}, bindings which express a symmetry constraint between two nodes
in a directed graph. Sourcing the first attribute would lead to
\texttt{[?a ?b]} prefixes, and vice versa. In both cases, the other
binding would never get a chance to participate in prefix extension.

In our current implementation, Hector detects all conflicts, but only
handles those with constant bindings. This is done by filtering the
source binding accordingly. \texttt{@TODO c.f. section on filter vs join for
filtering}.

For the following we will again assume that a suitable variable order
is at hand. We look at the variable order, and the variables bound by
the current prefix and determine from that the next variable $x$, to
which prefixes should be extended. Ignoring the source binding, we
then filter all other bindings down to only those that bind ("talk
about") $x$. Here we also skip bindings that are not \emph{ready} to
participate in prefix extension to $x$. We will describe this notion
in greater detail when discussing the problem of finding suitable
variable orderings. For now, we will try to provide some intuition.

\texttt{@TODO Example}

``` clojure
```

\subsection{Future Work}

Eventually we don't even want to call \texttt{count} on bindings that
we know should only ever constrain. [@TODO]

\subsection{Multi-tenant Routing}

\end{document}
