\documentclass[../index.tex]{subfiles}

\begin{document}

3DF builds on a rather recent body of work that allows us to even
consider the continuous maintenance of queries, whose complexity used
to be the exclusive domain of analytical databases. In this chapter we
describe the dataflow model of computation as a foundation for
stream-processing systems. We then provide intuition for the concept
of \emph{progress tracking}, and how it enables the efficient
execution of nested, cyclic dataflows. Finally, we discuss how
\emph{differential dataflow} extends the dataflow model to
\emph{incremental} operators and enables recursive queries through
independent, efficient iteration of regions of the dataflow.

\subsection{Dataflow}

\emph{Dataflow}, as commonly used, refers to two different but related
concepts, both of which are relevant to this work.

On the one hand, \emph{Dataflow} is an architectural paradigm in which
the execution of a computation is coordinated entirely via the
availability of data. This is in opposition to the von Neumann
architecture, which uses control structures (sequential statements,
branching via conditionals, loops) to coordinate the execution. Today,
we often describe systems that follow this idea as \emph{reactive}.

On the other hand, \emph{Dataflow programming} refers to a programming
paradigm, in which the structure of a computation is expressed as a
directed graph. This dataflow graph allows us to reason about
(distributed) execution of the computation. In particular, it allows
us to infer data dependencies and thus execution strategies that avoid
re-computation of unaffected paths through the graph on new inputs. In
the distributed setting, a fine-grained view of data dependencies
allows us to exploit more opportunities for concurrent execution.

\subsection{Progress Tracking}

\emph{Timely Dataflow} (\cite{timely}) is an implementation of the
cyclical dataflow model introduced in \cite{murray2013naiad}. It is
written in the Rust language. Timely Dataflow follows a data-parallel
approach, meaning each worker in a Timely cluster runs the same
dataflow computation with inputs partitioned between them.

Dataflow computations over unbounded, potentially out-of-order inputs
must reason about times at which it is safe to produce
results. Producing results over-eagerly will keep latencies low, but
causes downstream inconsistencies, because forwarded results do not
include all relevant inputs. Likewise, holding results back for too
long will ensure correctness but might increase latencies
unneccesarily (taken to the extreme, this end of the spectrum
corresponds to batch processing).

To solve this problem, a logical timestamp is attached to each datum
and a partial order is imposed on them. The ordering controls
visibility, thus detaching physical from logical availability. Taken
together, propagating information about input epochs along the
dataflow graph allows us to infer for which logical timestamps results
might safely be produced.

\emph{Progress tracking} is the problem of determining for each point
in the dataflow the set of input timestamps that may still be received
there. Timely Dataflow is a run-time for data-parallel dataflow
computations that coordinate via fine-grained progress tracking.

Without a co-ordination mechanism such as progress tracking, query
results will in general not reflect a consistent logical time.

\subsection{Differential Dataflow} \label{background-differential}

As the complexity of our computation grows and data volumes increase,
we will want to avoid re-computing results from scratch whenever an
input changes. Rather, we would like to incrementally update previous
results with (hopefully) small changes. In particular we would like to
do so even for iterative computations which often arise in graph
processing.

Differential Dataflow (\cite{differential}) is a programming framework
implementing differential dataflow (\cite{mcsherry2013differential})
on top of Timely. It allows to express dataflow programs as functional
transformations over \emph{collections} of data, using well-known
functional operators such as \texttt{map}, \texttt{filter}, and
\texttt{reduce}, and relational operators like
\texttt{join}. Differential also allows users to express iterative
computations via operators such as \texttt{iterate}.

Computations expressed in this model will be compiled down to Timely
dataflows that will update efficently (i.e. incrementally) in response
to arbitrary input changes.

Differential Dataflow also introduces the, to our knowledge novel,
concept of \emph{Arrangements}. Arrangements maintain indexed batches
of updates to a collection. The resulting indexed state can be shared
between operators and allows for the dynamic creation of dataflows
that can feed off of them.

Differential collections implement multi-set semantics.

\subsection{3DF}

With a robust implementation of differential dataflow available it
becomes feasible to re-think the boundaries between the domain of
analytical databases and that of near real-time stream processors.

3DF (\cite{declarative}, short for \emph{Declarative Differential
  Dataflow}) is an attribute-oriented data model and accompanying
query engine built on top of Differential Dataflow. Workers within a
3DF cluster will extend Timely workers with an event-driven server
loop that accepts commands from external clients. In particular,
clients can send query plans that are then synthesized into
differential dataflows at runtime.

We will work within the existing 3DF codebase throughout this
thesis. Thus, the next chapter is dedicated to describe the design and
implementation of 3DF at the outset of this work.

\end{document}
