\documentclass[../index.tex]{subfiles}

\begin{document}

\subsection{Rule Engines}

Rule-based systems allow for the expression of business rules, often
in a high-level language. Registered rules will then be continuously
evaluated against an input stream of business events. 

It quickly becomes infeasible to re-evaluate all rules on every
input. Specialized algorithms are employed to efficiently determine
the subset of rules that might possibly be affected by a new
input. Those rules are then re-evaluated. The RETE algorithm due to
Forgy (\cite{forgy1989rete}) is the archetypical pattern-matching
algorithm powering rule engines. PHREAK \cite{drools} is a
collection-oriented alternative to the tuple-by-tuple RETE.

At its core, RETE is an algorithm to compile rule definitions into a
corresponding \emph{discrimination network}. This approach is very
much reminiscent of a dataflow computation.

Most of the operators (called \emph{nodes}) that Forgy describes are
simple filters for an input's data type and similar discriminating
attributes. These are easily replicated by Differential's
\texttt{filter} operator. A clause such as \texttt{[?x :class/attr
    ?a]} will furthermore act as an inherent object-type filter.
Forgy also describes \texttt{join} nodes, which are easily emulated by
Differential's own \texttt{join} operator.

Critically, RETE networks assume that leaf nodes are traditional,
ad-hoc queries run against a conventional database. In Differential,
the entire computation is incrementalized.

While the literature on rule engines does provide interesting
approaches to incrementalizing traditional query engines, they lack
the relational abstractions, strong consistency guarantees, and the
ability to distribute recursive queries.

\subsection{Traditional Query Optimization}

The field of query optimization is vast. For the purposes of this work
we will be focusing mostly on relational query planning. Query
planning is the task of, given a tree structure of relational
operators, determining the most efficient order in which to execute
these operators without affecting the result set.

\cite{selinger1979access} give the canonical query planner, split into
three components: cardinality estimation, plan space enumeration, and
a cost model. This seminal work, done for the System R DBMS, first
established the practicality of declarative query languages and — to
this day — serves as the reference architecture for many modern
databases in productive use.

Selinger et al. introduce the concept of an \emph{access path}. Access
paths represent the different ways of accessing the tuples in a
relation. The simplest access path is a full scan of the relation, but
various indices might be available as well.

For queries that don't involve joins the System R planner employs a
cost model incorporating statistics gathered on the expected
selectivity of each access path available to service each of the
requested selection predicates. Additionally, the planner will check
whether the desired output ordering matches the order provided by any
suitable access path.

The complete cost model is somewhat more complex, because it allows
for a weighting between I/O and CPU cost and must deal with the fact
that memory segments will contain tuples from different
relations. Therefore the cost of scanning through a seemingly small
relation might be much higher in reality. Given this cost model, the
planner can pick the cheapest access path to service the query.

Of course, the more interesting case is that involving multiple
relations, which must be joined together. Given the cost model for
queries on individual relations, the planner now has to enumerate join
order permutations, compute overall cost, and pick the cheapest
overall plan. Simple heuristics are used to reduce the number of
permutations under consideration.

Many variants and improvements on this architecture have been proposed
over the years, most notably so in a long line of research by Graefe
et al.[1,2]. These architectures vary in their plan enumeration
strategies (bottom-up vs top-down) and improve the extensibility of
the planning architecture. The basic approach of heuristics-based cost
estimation combined with plan enumeration stays the same.

\begin{quote}
The root of all evil, the Achilles Heel of query optimization, is the
estimation of the size of intermediate results, known as
cardinalities. Everything in cost estimation depends upon how many
rows will be processed, so the entire cost model is predicated upon
the cardinality model.

\cite{lohman2014query}
\end{quote}

It is well established[3,4], that cardinality estimation is both the
most important part of traditional query optimization, as well as the
hardest to get right. \cite{leis2015good} show that estimation errors
grow exponentially with the number of joins for a representative
selection of modern relational DBMS. They also show that correct
cardinality estimates are much more important for plan performance
than more accurate cost models. Due to 3DFs highly normalized data
model, a large class of relevant queries require multi-way joins
across many base relations. This implies that 3DF would be exceedingly
sensitive to these kinds of ballooning errors.

Neither the importance of cardinality estimation, nor the difficulty
of doing it correctly are surprising. Until recently, most strategies
for planning multi-way joins consisted of breaking down an n-way join
into a (hopefully optimal) sequence of two-way joins. In such a
setting, accurate cardinality estimation of the result of joining two
relations (also known as the join's \emph{selectivity}) is crucial, in
order to pick a plan that materializes as few tuples as possible.

Many modern database systems still compute their estimates under the
same assumptions originally proposed for System R. In the following,
we want to review a few of those assumptions.

\subsubsection{Uniformity}

Consider a simple constant binding on a column \texttt{[?x
    :comment/browser Firefox]}, asking for user comments created using
the Firefox browser. Following \cite{selinger1979access}, the
selectivity of such a clause would be computed as

\begin{equation}
  \dfrac{1}{dom(\text{:comment/browser})} = \dfrac{1}{5}
\end{equation}

assuming five distinct browsers appear within our dataset. Crucially,
this formula assumes a uniform distribution of browsers, whereas we
would expect a distribution matching actual browser usage in our
target markets.

Given some additional index statistics, such as a histogram of the
actual frequencies, an optimizer might reasonibbly be expected to
determine a more accurate estimate.

However, as noted in \cite{lohman2014query}, queries in the real world
are often not written by humans directly but by other software
sytems. Such systems will often allow additional parameterization of
the query via a user interface. In the particular case of 3DF we will
often see the following transform, introducing an additional parameter
input:

\begin{verbatim}
[:find ?x :where [?x :comment/browser "Firefox"]]
=>
[:find ?x 
 :where [?x :comment/browser ?c] [_ :parameter ?c]]
\end{verbatim}

Within the continuous query processing setting, this allows users to
flip through different query parameters without having to create new
dataflows (and thus benefitting from the incremental re-computation of
results).

Unfortunately such an abstraction means, that we can not look up the
accurate selectivity during query registration, because the actual
parameter value is not known. Phrased differently, in our setting, the
problem of estimating selectivity of constant predicates is often
indistinguishable from estimating the selectivity of a join.

This issue also touches on a larger one, which is that traditional
optimizers assume that they have access to up-to-date, accurate
statistics on all relevant query inputs. This is less and less true in
today's architectures. 3DF's data model is built with heterogeneous,
external data sources in mind, for which suitable statistics might not
be available.

\subsubsection{Principle of Inclusion}

\cite{selinger1979access} give the selectivity of a relational
equi-join on two columns $c_1$ and $c_2$ as
$\dfrac{1}{max(dom(c_1),dom(c_2))}$. This assumes that the domains of
the two columns overlap. On otherwise unselected columns with
appropriate referential constraints between them, this assumption is
valid.

As soon as we apply other selection predicates before joining, this
assumption will not hold in general. For a single, binary join,
violating this assumption is usually not disastrous, because we 

This becomes problematic in
combination with the next assumption.

\subsubsection{Independent Predicates}

Multiple conjunctive predicates on the same relation are assumed to be
independent, allowing for their combined selectivity to be computed as
the product of their individual selectivities.

\subsection{Adaptive Query Processing}

We have seen in the previous chapter, that all traditional approaches
to cardinality estimation (and thus to join ordering and query
planning at large) are susceptible to exponentially compounding
estimation errors due to the use of heuristics.

A more recent field of research explored the idea, that if disastrous
plans would have to be expected from time to time, systems might be
able to detect them and adapt on the fly.

\cite{avnur2000eddies} introduced a new query processing architecture,
called \emph{Eddies}, which continuously reorders operators as it
runs. In a dataflow settings, this is best imagined as an additional
dataflow operator routing input tuples between a set of candidate
operators.

Such an architecture has additional benefits for long-running queries,
as we might not want to spend a lot of time on fine-grained planning
at registration time, but rather amortize the planning cost and adapt
as we gather more statistics about runtime behaviour.

\subsection{Worst-Case Optimal Join Processing}

@TODO How do eddies and WCO proposals relate?

\subsection{Stream Optimization}

\cite{hirzel2014catalog} helpfully provide a catalog of the most
common optimizations in stream processing, of which we will highlight
a few.

\textbf{Operator Reordering} is a much simpler version of the ideas
discussed in the previous chapters. The basic idea is to move more
selective operators upstream, in order to minimize the number of
tuples propagated downstream.

\textbf{Redundancy Elimination} attempts to remove redundant operators,
e.g. by moving equivalent operators on two branches of a dataflow into
their shared trunk.

\textbf{State Sharing} avoids unnecessary copies of data, by allowing
operators to access a shared piece of state (e.g. an index).

\textbf{Batching} exploits the fact that many operators can amortize
processing across many inputs.

An interesting trade-off between pushing selective operators upstream
and redundancy elimination arises in the multi-tenant setting, as the
more selective operators tend to differ between computations. Pushing
these operators upstream might thus severely reduce the re-usability
of a dataflow trunk, while significantly reducing the number of tuples
flowing downstream. 

Hirzel et al. again cite more sophisticated techniques to share
dataflow elements in a multi-tenant setting. We will come back to
these in chapter \ref{case-redundant-dataflows}.

A similar trade-off arises between separation of operators into
smaller computational steps in order to exploit pipeline parallelism,
and merging of operators into larger units, in order to minimize
communication and scheduling overheads. For example, multiple
transform or filter stages can be merged into a single one, by
composing their transformation functions / predicates, and vice versa.

Differential's arrangements provide a general purpose way to share
compacted and sorted batches of tuples between operators.

\subsection{Efficient Representations}

\cite{abadi2013design} note that data stored in columns is more
compressible than data stored in rows, because the former generally
have lower information entropy and better value locality. Having
established the column-inspired data model of 3DF in \ref{3df}, we are
now interested to what extent efficient, column-oriented
representations can be exploited in differential collections.

Again Abadi et al. list several common compression schemes employed in
modern column-stores:

1. Run length encoding
2. Bit-vector encoding
3. Dictionary encodings
4. Frame-of-reference encoding

Differential collections already employ a kind of run length encoding
scheme, by virtue of their multi-set semantics.

\subsubsection{Late Materialization}

@TODO
Another technique from the world of column-stores. It seems like a
generalization of predicate-pushdown. E.g. for certain simple
aggregates it should be possible to push-down the aggregation and do
it directly on a (potentially compressed) single column.

\subsubsection{Interning}

@TODO

\subsection{Conclusions}

We reiterate our desire for a practical system serving diverse data
needs in real-world organizations. Consequently, we care less about
picking optimal queries for every individual query and are more
interested in minimizing the number of disastrous plans chosen. Such
plans not only affect individual query performance, but negatively
impact all users of the system.

At a first glance, translating the System R approach to 3DF seems
straightforward. Materialized sub-computations (\emph{arrangements})
provide additional access-paths to tuples, for which a reasonable cost
model could be established. Systems such as \cite{condie2008evita}
provide interesting precedence of applying Datalog to the internals of
a query engine itself.

Arguably, the suitability of Differential to on-line analytical
computations would ease some of the challenges of maintaining
up-to-date metrics on relations, without incurring a significant
operational overhead. Yet, we do not see a promising avenue to tackle
the fragility of cardinality estimation (as discussed in the previous
sections) on complex queries invoving many joins using fundamentally
heuristics-based techniques.

Even worse, subgraph queries will often not have \texttt{@TODO}

The novel execution model of differential dataflows and the highly
dynamic environments 3DF is designed for do not bear much resemblance
to the operational realities 40 years ago. Traditional planners are
not built with incremental, streaming evaluation in mind.

@TODO Sources

[0] [Selinger et al., Access Path Selection in a RDMS](../sources/access-path-selection.pdf)
[1] [Graefe, McKenna, The Volcano Optimizer Generator](../sources/volcano.pdf)
[2] [Graefe, The Cascades Framework For Query Optimization](../sources/the-cascades-framework-for-query-optimization.pdf)
[3] [Leis et al., How Good Are Query Optimizers, Really?](../sources/how-good-are-optimizers-really.pdf)
[4] [Lohman, Is Query Optimization A Solved Problem?](../sources/is-query-optimization-a-solved-problem.pdf)
[5] [Hellerstein, Query Optimization (Red Book, 5th ed.)](http://www.redbook.io/ch7-queryoptimization.html)

\end{document}
