\documentclass[../index.tex]{subfiles}

\begin{document}

Over the previous chapters we have given arguments for why
Differential Dataflow poses a unique opportunity to build a system
that combines the performance characteristics of distributed stream
processing with the analytical power of relational and graph
databases. We have also introduced 3DF as a prototype of such a
system.

In this chapter we will survey the decades of research on optimizing
both, relational query processing and streaming computations, and
discuss to what extent these techniques could aid us in making 3DF fit
for use in highly dynamic, multi-user environments.

\subsection{Cost-Based Query Planning}

The field of relational query planning is vast. Query planning is the
problem of, given a tree structure of relational operators,
determining the most efficient order in which to execute these
operators without affecting the result set.

\cite{selinger1979access} give the canonical query planner, split into
three components: cardinality estimation, plan space enumeration, and
a cost model. This seminal work, done for the System R DBMS, first
established the practicality of declarative query languages and — to
this day — serves as the reference architecture for many modern
databases in productive use.

Selinger et al. introduce the concept of an \emph{access path}. Access
paths represent the different ways of accessing the tuples in a
relation. The simplest access path is a full scan of the relation, but
various indices might be available as well.

For queries that don't involve joins the System R planner employs a
cost model incorporating statistics gathered on the expected
selectivity of each access path available to service each of the
requested selection predicates. Additionally, the planner will check
whether the desired output ordering matches the order provided by any
suitable access path.

The complete cost model is somewhat more complex, because it allows
for a weighting between I/O and CPU cost and must deal with the fact
that memory segments will contain tuples from different
relations. Therefore the cost of scanning through a seemingly small
relation might be much higher in reality. Given this cost model, the
planner can pick the cheapest access path to service the query.

Of course, the more interesting case is that involving multiple
relations, which must be joined together. Given the cost model for
queries on individual relations, the planner now has to enumerate join
order permutations, compute overall cost, and pick the cheapest
overall plan. Simple heuristics are used to reduce the number of
permutations under consideration.

Many variants and improvements on this architecture have been proposed
over the years, most notably so in a long line of research by Graefe
et al. (\cite{graefe1987exodus}, \cite{graefe1993volcano}, and
\cite{graefe1995cascades}). These architectures vary in their plan
enumeration strategies (bottom-up vs top-down) and improve the
extensibility of the planning architecture. The basic approach of
heuristics-based cost estimation combined with plan enumeration stays
the same.

\subsection{Cardinality Estimation}

\begin{quote}
The root of all evil, the Achilles Heel of query optimization, is the
estimation of the size of intermediate results, known as
cardinalities. Everything in cost estimation depends upon how many
rows will be processed, so the entire cost model is predicated upon
the cardinality model.

\cite{lohman2014query}
\end{quote}

It is well established, that cardinality estimation is both the most
important part of cost-based query planning, as well as the hardest to
get right. \cite{leis2015good} show that estimation errors grow
exponentially with the number of joins for a representative selection
of modern relational DBMS. They also show that correct cardinality
estimates are much more important for plan performance than more
accurate cost models. Due to 3DFs highly normalized data model, a
large class of relevant queries require multi-way joins across many
base relations. This implies that 3DF would be exceedingly sensitive
to these kinds of ballooning errors.

Neither the importance of cardinality estimation, nor the difficulty
of doing it accurately are surprising. Until recently, most strategies
for planning multi-way joins consisted of breaking down a n-way join
into a (hopefully optimal) sequence of two-way joins. In such a
setting, accurately estimating the number of tuples contained in the
result set of joining two relations is crucial in order to pick a plan
that materializes as few tuples as possible. Even slightly
over-optimistic selectivity estimates can cause cardinality estimation
errors proportional in the size of the input relations.

Many modern database systems still compute their estimates under the
same assumptions originally proposed for System R.

\begin{itemize}
\item \textbf{Uniformity}

  Uniformity is the assumption, that the values in an attribute are
  uniformly distributed. Under this assumption, the selectivity of
  predicates on any specific value is computed simply as the inverse
  of the domain cardinality of the attribute. This is of course a
  naive assumption, as attributes in the real-world tend to exhibit a
  certain bias, e.g. age and location of users of a specific service.

\item \textbf{Principle of Inclusion}

  \cite{selinger1979access} give the selectivity of a relational
  equi-join on two columns as the inverse of the greater of their
  domain cardinalities. This assumes that the domains of the two
  columns overlap. On otherwise unselected columns with appropriate
  referential constraints between them, this assumption is valid. As
  soon as we apply other selection predicates before joining, this
  assumption will not hold in general.

\item \textbf{Independence}

  Multiple conjunctive predicates on the same relation are assumed to
  be uncorrelated, allowing for their combined selectivity to be
  computed as the product of their individual selectivities. While at
  first glance, with the good relational practice of normalizing data
  models in mind, this seems like a reasonable assumption, it quickly
  breaks down on real-world data. A simple example might be the two
  attributes ``age'' and ``salary'', which tend to correlate in most
  organizations. Breaking such functional dependencies down into more
  fundamental, independent attributes and combining them with
  intricate models necessary to re-derive salaries is not feasible in
  practice. Often relations are the most succinct and pragmatic way to
  model a functional dependency.

\item \textbf{Global Information}

  It is assumed that the query planner has access to up-to-date,
  accurate statistics on all relevant query inputs. This critical
  assumption does not hold in the continuous query processing setting,
  where not a single relevant input might have yet been seen at
  planning time.

\end{itemize}

While more sophisticated statistical models and methods have been
proposed to loosen the reliance on assumptions of uniformity and
independence (\cite{markl2007consistent}, \cite{ilyas2004cords}), it
is the assumption of global information that prevents the use of
traditional cardinality estimation when maintaining queries over
potentially unbounded streams.

\subsection{Adaptive Query Processing} \label{technique-adaptive}

In the previous section we have seen that traditional approaches to
cardinality estimation (and thus to join ordering and query planning
at large) are susceptible to exponentially compounding estimation
errors due to the use of simplistic heuristics and furthermore can not
be translated into the continuous query processing setting, due to the
unavailability of \emph{any} meaningful statistics.

An additional problem arises in this context, because even when
accurate heuristics are available at query planning time, they might
bear very little resemblance to those gathered after a few hours of
new inputs. Therefore any planning decisions made based on those
assumptions will have to be revised over time. In a general purpose,
multi-user environment, queries might live anywhere from a few seconds
to months or even years — ample time for the underlying distributions
to change dramatically.

Adaptive query processing explores the idea, that in environments
where disastrous plans are to be expected every once in a while, and
planning assumptions change constantly over time, then systems should
be able to adapt planning decisions on the fly.

\cite{avnur2000eddies} introduce a new query processing architecture,
called \emph{eddy}, which ``continuously reorders operators in a query
plan as it runs''. In a dataflow settings, this is best imagined as an
additional dataflow operator routing input tuples between a set of
candidate operators. The routing logic is then adapted, based on the
measured flow of tuples from each of the candidate operators.

A significant challenge in \cite{avnur2000eddies} is the problem of
identifying \emph{moments of symmetry}, synchronization points within
the processing pipeline, at which operator inputs can be re-ordered
without the need to modify the accumulated operator state (which can
be on the order of the entire input relation for joins). They analyse
various families of join algorithms under this new
criterium. Differential's incremental join operator is entirely
co-ordinated by its inputs and, in combination with logical
timestamps, might not necessarily be affected by the same
considerations. However an exact translation of eddies into
Differential would require more investigation.

A complementary approach to the adaptive query processing problem is
that of \emph{progressive optimization}, as employed by
\cite{markl2004robust}. This work extends the query processing
pipeline with a re-optimization step, validating cardinality estimates
against the actual values measured during execution of a previous
query. Re-optimization imposes a synchronization barrier on the
processing pipeline, which is inappropriate for long-lived,
low-latency dataflows.

While the specifics of an adaptive query processing primitive would
therfore likely be different in Differential Dataflow, the motivating
observations clearly apply.

\subsection{Worst-Case Optimal Join Processing} \label{technique-wco}

Taken to their logical conclusion, the ideas from adaptive query
processing lead to the nascent field of worst-case optimal join
processing. Here the fundamental observation is, that the traditional
approach to multi-way joins (consecutive two-way joins, or
\emph{join-at-a-time}) will always be sub-optimal for certain queries,
because they generate intermediate results that can be larger than the
final result set.

The initial work on worst-case optimal join algorithms is due to
\cite{ngo2012worst}, with independent results by
\cite{veldhuizen2012leapfrog}. Both of these algorithms (called
\emph{NPRR} and \emph{Leapfrog TrieJoin} respectively) avoid
generating intermediate results and are thus considered to process
n-way joins all at once, instead of join-at-a-time. In fact, both were
later generalized under a common framework in \cite{ngo2013skew},
called \emph{GenericJoin}.

Given some ordered sequence of target variables and a set of
relations, GenericJoin proceeds \emph{variable-by-variable} rather
than relation-by-relation, starting from the set containing only the
empty prefix. From a set of prefixes, GenericJoin iteratively computes
the \emph{extension} of that set to the next variable in the variable
ordering.

For each such extension stage, candidate extensions will be drawn from
whichever relation promises to propose the least number of extensions,
and intersected with the would-be proposals from all other
relations. Assuming suitable indices on the relations, determining the
number of proposals for a relation and a given prefix can be done in
constant or logarithmic time. Similarly, intersecting a proposed
extension with a relation's proposal index can be done in constant or
logarithmic time. Additionally, proposals must be provided with
constant delay.

Under these assumptions, each extension stage of a GenericJoin will
only do work on the order of the largest possible result set. Picking
suitable variable orderings is a crucial part of these new
algorithms. While sub-optimal orderings do not affect the theoretical
guarantees, they can make a significant difference in
practice. Intuitively, this is because the maximum size of the result
set is itself dependent on the variable ordering.

\cite{ciucanu2015worst} apply ideas from worst-case optimal join
within the framework of \emph{factorized databases}. They show, that
worst-case optimal complexity can be achieved with join-at-a-time
plans as well, by eliminating redundancy from the representation of
intermediate results.

More recently, \cite{ammar2018distributed} give worst-case optimal
join algorithms for the data-parallel dataflow setting. The resulting
operators are very much reminiscent of eddies, in that they make
planning decisions on a per-tuple basis. They also extend the
worst-case optimality to cover communication costs between workers.

Worst-case optimal join algorithms are therefore promising for two
reasons. One, they extend the class of queries for which we can hope
to find non-disastrous plans and for which no optimal two-way join
ordering exists. Second, they implement a very granular,
non-heuristical form of adaptive query processing without the need for
synchronized re-optimization points. We make use of an adapted version
of the worst-case optimal join algorithm by
\cite{ammar2018distributed} in \autoref{case-join-ordering}, in order
to provide predictable query performance without relying on
heuristics.

\subsection{Continuous Query Processing}

Continuously evaluating queries over data streams using incremental
computation is of course not a new idea, and commonly known as
\emph{incremental view maintenance}. Early treatments such as
\cite{terry1992continuous} identify the shortcomings of
non-incremental, periodic execution of queries, propose continuous
incremental maintenance and identify interesting semantics aspects,
such as the need for stratified negation. Their \emph{Tapestry} system
however supports only monotonic queries over append-only workloads on
a single thread. Differential Dataflow provides the same incremental
semantics while supporting retractions and distributed execution.

\cite{blakeley1986efficiently} introduce the concept of
\emph{differential re-evaluation} of views. In particular, they
exploit the associativity of the join operator and observe that, for
changes to a single relation, $A' \bowtie B = (A + \delta{a}) \bowtie
B = (A \bowtie B) + (\delta{a} \bowtie B)$ — implying that the
relational join operator is already incrementalized w.r.t. changes to
any individual of its inputs.

Further, because of the distributivity of the join operator, the union
of multiple such views can be constructed to arrive at an
incrementalized join operation w.r.t. to arbitrary changes to
\emph{any} of its inputs.

We adopt this approach in \autoref{case-join-state}, in order to
incrementally process high-arity joins with only a constant memory
footprint in addition to the arranged base relations.

\subsection{Stream Optimization}

We turn now away from optimization as practiced by the database
community, and survey the state of general stream processing
optimizations. \cite{hirzel2014catalog} provide a catalog of the most
common optimizations in stream processing, of which we will highlight
a few.

\begin{itemize}
  \item \textbf{Operator Reordering} is a much simpler version of the
    ideas discussed in the previous chapters. The basic idea is to
    move more selective operators upstream, in order to minimize the
    number of tuples propagated downstream. Selectivity in this
    setting is treated mostly at the level of operator types
    (e.g. \texttt{Filter} has a higher selectivity than \texttt{Map}).

  \item \textbf{Redundancy Elimination} attempts to remove redundant
    operators, e.g. by moving equivalent operators on two branches of
    a dataflow into their shared trunk.

  \item \textbf{State Sharing} avoids unnecessary copies of data, by
    allowing operators to access a shared piece of state, e.g. an
    index.

  \item \textbf{Batching} exploits the fact that many operators can
    amortize processing across many inputs.
\end{itemize}

An interesting trade-off between pushing selective operators upstream
and redundancy elimination arises in the multi-tenant setting, as the
more selective operators tend to differ between computations. Pushing
these operators upstream might thus severely reduce the re-usability
of a dataflow trunk, while significantly reducing the number of tuples
flowing downstream. Hirzel et al. cite more sophisticated techniques
to share dataflow elements in a multi-tenant setting. We will come
back to these in \autoref{case-redundant-dataflows}.

A similar trade-off arises between separation of operators into
smaller computational steps in order to exploit pipeline parallelism,
and merging of operators into larger units, in order to minimize
communication and scheduling overheads. For example, multiple
transform or filter stages can be merged into a single one, by
composing their transformation functions / predicates, and vice versa.

Most of the techniques explored in this line of inquiry are geared
towards simpler computational frameworks than the relational model and
its extensions. Additionally, Differential's arrangements already
provide a general purpose way to share compacted and sorted batches of
tuples between operators.

Therefore, while micro optimizations on the dataflow graph can be
beneficial, we are more concerned with optimizations that affect the
overall system design and have the potential to provide asymptotic
improvements.

\subsection{Rule Engines}

Finally, we look into the intersectional topic of rule
engines. Rule-based systems allow for the expression of business
rules, often in a high-level language. Registered rules will then be
continuously evaluated against an input stream of business events. It
quickly becomes infeasible to re-evaluate all rules on every
input. Specialized algorithms are employed to efficiently determine
the subset of rules that might possibly be affected by a new
input. Those rules are then re-evaluated.

The RETE algorithm due to Forgy (\cite{forgy1989rete}) is the
archetypical pattern-matching algorithm powering rule engines. At its
core, RETE is an algorithm to compile rule definitions into a
corresponding \emph{discrimination network}. This approach is very
much reminiscent of a dataflow computation.

Most of the operators (called \emph{nodes}) that Forgy describes are
simple filters for an input's data type and similar discriminating
attributes. These are easily replicated by Differential's
\texttt{filter} operator. A clause such as \texttt{[?x :class/attr
    ?a]} will furthermore act as an inherent object-type filter.
Forgy also describes \texttt{join} nodes, which are easily emulated by
Differential's own \texttt{join} operator.

Critically, RETE networks assume that leaf nodes are traditional,
ad-hoc queries run against a conventional database. In Differential,
the entire computation is incrementalized. While the literature on
rule engines does provide interesting approaches to incrementalizing
traditional query engines, they lack the relational abstractions,
strong consistency guarantees, and the ability to distribute recursive
queries.

\subsection{Conclusions}

We reiterate our desire for a practical system serving diverse data
needs in real-world organizations. Consequently, we care less about
picking optimal queries for every individual query and are more
interested in minimizing the number of disastrous plans chosen. Such
plans not only affect individual query performance, but negatively
impact all users of the system.

At a first glance, translating the System R approach to 3DF seems
straightforward. Materialized sub-computations (via Differential's
arrangements) could provide additional access-paths to tuples, for
which a reasonable cost model could be established. Systems such as
\cite{condie2008evita} provide interesting precedence of applying
Datalog to the internals of a query engine itself. Arguably, the
suitability of Differential to on-line analytical computations would
ease some of the challenges of maintaining up-to-date metrics on
relations, without incurring a significant operational overhead.

But we have also seen that 3DF's continuous query processing model
breaks crucial assumptions made by traditional planners to an extent,
that we do not see a heuristics based approach as viable. Summarizing
the above sections, the reasons for this are manifold:

\begin{enumerate}
  \item
    Cardinality estimation is simultaneously the most important and
    the most fragile part of query planning. Estimation errors are
    often proportional in the size of entire relations and compound on
    complex queries invoving many joins. We saw that estimation errors
    of many orders of magnitude are not merely a theoretical concern
    but a common occurrence in real-world planners.

  \item
    In our setting, we must implement computations \emph{before} any
    data (and thus any heuristics) are available. Data sources are
    heterogeneous, live outside of 3DF and will often not provide the
    necessary statistics. This invalidates the assumption of
    global-knowledge that traditional query planners operate under.

  \item
    Even in clean, normalized data models, certain correlations
    between columns are near impossible to avoid in practice, because
    a relation is often the only feasible way of modeling discrete,
    functional dependencies.

  \item
    Even assuming accurate and timely metrics, certain queries (such
    as the subgraph queries from section \ref{technique-wco}) have
    \emph{no} join-at-a-time plan that materializes no more tuples
    than the maximum result set size.
\end{enumerate}

Fortunately we have also seen that more recent approaches from the
field of adaptive query processing, incremental view maintenance, and
worst-case optimal join processing match our operational needs much
more closely.

In the following chapters, we will now focus on what we believe to be
the most significant challenges facing any system that wishes to
provide continuously evaluated, complex relational queries to many
interactive clients with predictable, consistently good
performance. For each we will describe the problem, first in general
terms and then alongside specific examples. We will then describe and
evaluate potential remedies, based on the techniques we have explored
in this chapter.

\end{document}
