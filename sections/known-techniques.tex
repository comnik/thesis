\documentclass[../index.tex]{subfiles}

\begin{document}

\subsection{Rule Engines}

Rule-based systems allow for the expression of business rules, often
in a high-level language. Registered rules will then be continuously
evaluated against an input stream of business events. 

It quickly becomes infeasible to re-evaluate all rules on every
input. Specialized algorithms are employed to efficiently determine
the subset of rules that might possibly be affected by a new
input. Those rules are then re-evaluated. The RETE algorithm due to
Forgy (\cite{forgy1989rete}) is the archetypical pattern-matching
algorithm powering rule engines. PHREAK \cite{drools} is a
collection-oriented alternative to the tuple-by-tuple RETE.

At its core, RETE is an algorithm to compile rule definitions into a
corresponding \emph{discrimination network}. This approach is very
much reminiscent of a dataflow computation.

Most of the operators (called \emph{nodes}) that Forgy describes are
simple filters for an input's data type and similar discriminating
attributes. These are easily replicated by Differential's
\texttt{filter} operator. A clause such as \texttt{[?x :class/attr
    ?a]} will furthermore act as an inherent object-type filter.
Forgy also describes \texttt{join} nodes, which are easily emulated by
Differential's own \texttt{join} operator.

Critically, RETE networks assume that leaf nodes are traditional,
ad-hoc queries run against a conventional database. In Differential,
the entire computation is incrementalized.

While the literature on rule engines does provide interesting
approaches to incrementalizing traditional query engines, they lack
the relational abstractions, strong consistency guarantees, and the
ability to distribute recursive queries.

\subsection{Traditional Query Optimization}

The field of query optimization is vast. For the purposes of this work
we will be focusing mostly on relational query planning. Query
planning is the task of, given a tree structure of relational
operators, determining the most efficient order in which to execute
these operators without affecting the result set.

\cite{selinger1979access} give the canonical query planner, split into
three components: cardinality estimation, plan space enumeration, and
a cost model. This seminal work, done for the System R DBMS, first
established the practicality of declarative query languages and — to
this day — serves as the reference architecture for many modern
databases in productive use.

Selinger et al. introduce the concept of an \emph{access path}. Access
paths represent the different ways of accessing the tuples in a
relation. The simplest access path is a full scan of the relation, but
various indices might be available as well.

For queries that don't involve joins the System R planner employs a
cost model incorporating statistics gathered on the expected
selectivity of each access path available to service each of the
requested selection predicates. Additionally, the planner will check
whether the desired output ordering matches the order provided by any
suitable access path.

The complete cost model is somewhat more complex, because it allows
for a weighting between I/O and CPU cost and must deal with the fact
that memory segments will contain tuples from different
relations. Therefore the cost of scanning through a seemingly small
relation might be much higher in reality. Given this cost model, the
planner can pick the cheapest access path to service the query.

Of course, the more interesting case is that involving multiple
relations, which must be joined together. Given the cost model for
queries on individual relations, the planner now has to enumerate join
order permutations, compute overall cost, and pick the cheapest
overall plan. Simple heuristics are used to reduce the number of
permutations under consideration.

Many variants and improvements on this architecture have been proposed
over the years, most notably so in a long line of research by Graefe
et al. (\cite{graefe1987exodus}, \cite{graefe1993volcano}, and
\cite{graefe1995cascades}). These architectures vary in their plan
enumeration strategies (bottom-up vs top-down) and improve the
extensibility of the planning architecture. The basic approach of
heuristics-based cost estimation combined with plan enumeration stays
the same.

\subsection{Cardinality Estimation}

\begin{quote}
The root of all evil, the Achilles Heel of query optimization, is the
estimation of the size of intermediate results, known as
cardinalities. Everything in cost estimation depends upon how many
rows will be processed, so the entire cost model is predicated upon
the cardinality model.

\cite{lohman2014query}
\end{quote}

It is well established, that cardinality estimation is both the most
important part of traditional query optimization, as well as the
hardest to get right. \cite{leis2015good} show that estimation errors
grow exponentially with the number of joins for a representative
selection of modern relational DBMS. They also show that correct
cardinality estimates are much more important for plan performance
than more accurate cost models. Due to 3DFs highly normalized data
model, a large class of relevant queries require multi-way joins
across many base relations. This implies that 3DF would be exceedingly
sensitive to these kinds of ballooning errors.

Neither the importance of cardinality estimation, nor the difficulty
of doing it correctly are surprising. Until recently, most strategies
for planning multi-way joins consisted of breaking down an n-way join
into a (hopefully optimal) sequence of two-way joins. In such a
setting, accurate cardinality estimation of the result of joining two
relations (also known as the join's \emph{selectivity}) is crucial, in
order to pick a plan that materializes as few tuples as possible.

Many modern database systems still compute their estimates under the
same assumptions originally proposed for System R. In the following,
we want to review a few of those assumptions.

\subsubsection{Uniformity}

Consider a simple constant binding on a column \texttt{[?x
    :comment/browser Firefox]}, asking for user comments created using
the Firefox browser. Following \cite{selinger1979access}, the
selectivity of such a clause would be computed as

\begin{equation}
  \dfrac{1}{dom(\text{:comment/browser})} = \dfrac{1}{5}
\end{equation}

assuming five distinct browsers appear within our dataset. Crucially,
this formula assumes a uniform distribution of browsers, whereas we
would expect a distribution matching actual browser usage in our
target markets.

Given some additional index statistics, such as a histogram of the
actual frequencies, an optimizer might reasonibbly be expected to
determine a more accurate estimate.

However, as noted in \cite{lohman2014query}, queries in the real world
are often not written by humans directly but by other software
sytems. Such systems will often allow additional parameterization of
the query via a user interface. In the particular case of 3DF we will
often see the following transform, introducing an additional parameter
input:

\begin{verbatim}
[:find ?x :where [?x :comment/browser "Firefox"]]
=>
[:find ?x 
 :where [?x :comment/browser ?c] [_ :parameter ?c]]
\end{verbatim}

Within the continuous query processing setting, this allows users to
flip through different query parameters without having to create new
dataflows (and thus benefitting from the incremental re-computation of
results).

Unfortunately such an abstraction means, that we can not look up the
accurate selectivity during query registration, because the actual
parameter value is not known. Phrased differently, in our setting, the
problem of estimating selectivity of constant predicates is often
indistinguishable from estimating the selectivity of a join.

This issue also touches on a larger one, which is that traditional
optimizers assume that they have access to up-to-date, accurate
statistics on all relevant query inputs. This is less and less true in
today's architectures. 3DF's data model is built with heterogeneous,
external data sources in mind, for which suitable statistics might not
be available.

\subsubsection{Principle of Inclusion}

\cite{selinger1979access} give the selectivity of a relational
equi-join on two columns $c_1$ and $c_2$ as
$\dfrac{1}{max(dom(c_1),dom(c_2))}$. This assumes that the domains of
the two columns overlap. On otherwise unselected columns with
appropriate referential constraints between them, this assumption is
valid.

As soon as we apply other selection predicates before joining, this
assumption will not hold in general. For a single, binary join,
violating this assumption is usually not disastrous, because we 

This becomes problematic in
combination with the next assumption.

\subsubsection{Independent Predicates}

Multiple conjunctive predicates on the same relation are assumed to be
independent, allowing for their combined selectivity to be computed as
the product of their individual selectivities.

\subsection{Adaptive Query Processing}

We have seen in the previous chapter, that all traditional approaches
to cardinality estimation (and thus to join ordering and query
planning at large) are susceptible to exponentially compounding
estimation errors due to the use of heuristics.

An additional problem arises in the context of continuously evaluating
queries over data streams, because the heuristics available at query
registration time might bear very little resemblance to those gathered
after a few hours of new inputs. Therefore any planning decisions made
based on those assumptions will have to be revised over time.

Adaptive query processing explores the idea, that if disastrous plans
are to be expected every once in a while, and planning assumptions
change over time, then systems should be able to adapt planning
decisions on the fly.

\cite{avnur2000eddies} introduced a new query processing architecture,
called \emph{eddy}, which ``continuously reorders operators in a query
plan as it runs''. In a dataflow settings, this is best imagined as an
additional dataflow operator routing input tuples between a set of
candidate operators. The routing logic is then adapted, based on the
measured flow of tuples from each of the candidate operators.

A significant challenge in \cite{avnur2000eddies} is the problem of
identifying \emph{moments of symmetry}, synchronization points within
the processing pipeline, at which operator inputs can be re-ordered
without the need to modify the accumulated operator state (which can
be on the order of the entire input relation for joins).

A complementary approach to the adaptive query processing problem is
\emph{progressive optimization}, as employed by
\cite{markl2004robust}. This work extends the query processing
pipeline with a re-optimization step, validating cardinality estimates
against the actual values measured during execution of a previous
query. Re-optimization imposes a synchronization barrier on the
processing pipeline, which is inappropriate for the continuous
setting.

Further, in real-world usage we can hardly avoid correlated columns,
because they are the only practical tool to model discrete, functional
dependencies. This largely invalidates the independence assumption of
join predicates.

Finally, any slightly over-optimistic estimation will immediately
cause errors proportional in the size of the relation, because
[@TODO].

In summary, we must conclude that cardinality estimation can not avoid
disastrous plans in real-world usage.

\subsection{Worst-Case Optimal Join Processing} \label{technique-wco}

Taken to their logical conclusion, the ideas from adaptive query
processing lead to the nascent field of worst-case optimal join
processing. Here the fundamental observation is, that the traditional
approach to multi-way joins (consecutive two-way joins, or
\emph{join-at-a-time}) will always be sub-optimal for certain queries,
because they generate intermediate results that can be larger than the
final result set.

The initial work on worst-case optimal join algorithms is due to
\cite{ngo2012worst}, with independent results by
\cite{veldhuizen2012leapfrog}. Both of these algorithms (called
\emph{NPRR} and \emph{Leapfrog TrieJoin} respectively) avoid
generating intermediate results and are thus considered to process
n-way joins all at once, instead of join-at-a-time. In fact, both were
later generalized under a common framework in \cite{ngo2013skew}.

Given some ordered sequence of target variables (e.g. \texttt{[?a ?b
    ?c]}) and a set of relations (e.g. \texttt{{[?a :edge ?b] [?b
      :edge ?c] [?c :edge ?a]}}) both algorithms proceed
\emph{variable-by-variable}, not relation-by-relation. They will for
example start by determining the set of all possible bindings of
\texttt{[?a]}, they might first determine all possible extensions to
\texttt{[?a ?b]} @TODO

\cite{ciucanu2015worst} apply ideas from worst-case optimal join
within the framework of \emph{factorized databases}. They show, that
worst-case optimal complexity can be achieved with join-at-a-time
plans as well, by eliminating redundancy from the representation of
intermediate results.

More recently, \cite{ammar2018distributed} give worst-case optimal
join algorithms for the data-parallel dataflow setting. The resulting
operators are very much reminiscent of eddies, in that they make
planning decisions on a per-tuple basis. However, they do not require
moments of symmetry nor synchronized re-optimization points. They also
extend the worst-case optimality to cover communication costs between
workers.

\subsection{Stream Optimization}

\cite{hirzel2014catalog} helpfully provide a catalog of the most
common optimizations in stream processing, of which we will highlight
a few.

\textbf{Operator Reordering} is a much simpler version of the ideas
discussed in the previous chapters. The basic idea is to move more
selective operators upstream, in order to minimize the number of
tuples propagated downstream. Selectivity in this setting is treated
mostly at the level of operator types (e.g. \texttt{Filter} has a
higher selectivity than \texttt{Map}).

\textbf{Redundancy Elimination} attempts to remove redundant operators,
e.g. by moving equivalent operators on two branches of a dataflow into
their shared trunk.

\textbf{State Sharing} avoids unnecessary copies of data, by allowing
operators to access a shared piece of state (e.g. an index).

\textbf{Batching} exploits the fact that many operators can amortize
processing across many inputs.

An interesting trade-off between pushing selective operators upstream
and redundancy elimination arises in the multi-tenant setting, as the
more selective operators tend to differ between computations. Pushing
these operators upstream might thus severely reduce the re-usability
of a dataflow trunk, while significantly reducing the number of tuples
flowing downstream. 

Hirzel et al. cite more sophisticated techniques to share dataflow
elements in a multi-tenant setting. We will come back to these in
chapter \ref{case-redundant-dataflows}.

A similar trade-off arises between separation of operators into
smaller computational steps in order to exploit pipeline parallelism,
and merging of operators into larger units, in order to minimize
communication and scheduling overheads. For example, multiple
transform or filter stages can be merged into a single one, by
composing their transformation functions / predicates, and vice versa.

Most of the techniques explored in this line of inquiry are geared
towards simpler computational frameworks than the relational model and
its extensions. Additionally, Differential's arrangements already
provide a general purpose way to share compacted and sorted batches of
tuples between operators.

In conclusion, while micro optimizations on the dataflow graph can be
very beneficial, we are more concerned with optimizations that affect
the overall system design and provide asymptotic improvements.

\subsection{Conclusions}

We reiterate our desire for a practical system serving diverse data
needs in real-world organizations. Consequently, we care less about
picking optimal queries for every individual query and are more
interested in minimizing the number of disastrous plans chosen. Such
plans not only affect individual query performance, but negatively
impact all users of the system.

At a first glance, translating the System R approach to 3DF seems
straightforward. Materialized sub-computations (via Differential's
arrangements) could provide additional access-paths to tuples, for
which a reasonable cost model could be established. Systems such as
\cite{condie2008evita} provide interesting precedence of applying
Datalog to the internals of a query engine itself. Arguably, the
suitability of Differential to on-line analytical computations would
ease some of the challenges of maintaining up-to-date metrics on
relations, without incurring a significant operational overhead.

But we have also seen that 3DF's continuous query processing model
breaks crucial assumptions made by traditional planners to an extent,
that we do not see a heuristics based approach as viable. Summarizing
the above sections, the reasons for this are manifold:

\begin{enumerate}
  \item
    Cardinality estimation is simultaneously the most important and
    the most fragile part of query planning. Estimation errors are
    often proportional in the size of entire relations and compound on
    complex queries invoving many joins. We saw that estimation errors
    of many orders of magnitude are not merely a theoretical concern
    but a common occurrence in real world planners.

  \item
    In our setting, we must implement computations \emph{before} any
    data (and thus any heuristics) are available. Data sources are
    heterogeneous, live outside of 3DF and will often not provide the
    neccessary statistics. This invalidates the assumption of
    global-knowledge that traditional query planners operate under.

  \item
    Even in clean, normalized data models, certain correlations
    between columns are near impossible to avoid in practice, because
    a relation is often the only feasible way of modeling discrete,
    functional dependencies.

  \item
    Even assuming accurate and timely metrics, certain queries (such
    as the subgraph queries from section \ref{technique-wco}) have
    \emph{no} join-at-a-time plan that materializes no more tuples
    than the maximum result set size.
\end{enumerate}

Fortunately we have also seen that more recent approaches from the
field of adaptive query processing and worst-case optimal join
processing match our operational needs much more closely.

\end{document}
