\documentclass[../index.tex]{subfiles}

\begin{document}

A field as vast as query optimization and the novel computational
model that Differential Dataflow presents make it impossible to cover
all relevant aspects and questions to a satisfactory degree. In this
chapter we will touch upon insights and questions for future research
that were obtained over the course of this work, without necessarily
fitting its primary theme.

\subsection{Automated Policies}

We have pointed out significant challenges to continuously evaluated,
complex relational query processing in an interactive, multi-user
environment. While we described, implemented, and evaluated effective
remedies, we have stopped short of providing a fully automated
framework for making planning decisions based on these
insights. Providing a fully automated planner is of course essential
to any real-world system and will thus be of great interest to us in
future work.

\subsection{Hybrid Approaches}

Throughout this work we have discussed the severe downsides of
cost-based optimization and have accordingly made efforts to build on
entirely different primitives. Nevertheless, exploring the interplay
between worst-case optimal algorithms and traditional join processing
within a hybrid cost-model warrants attention. A recent paper by
\cite{mhedhbi2019optimizing} introduces such a hybrid planner
architecture, with promising results.

\subsection{Adaptive Dataflows}

Early on in section \ref{technique-adaptive} we described the eddy
operator from \cite{avnur2000eddies}. We believe that the primary use
case of eddies, adaptive join orderings, is better served by
data-parallel worst-case optimal join algorithms as implemented in
this work, because they do not require additional synchronization
logic and provide provably optimal bounds.

However, adaptive routing between dataflows could be a complementary
technique to avoid the overheads of worst-case optimal join processing
(both in terms of dataflow size and best-case latencies). In
combination with flow control techniques such as those developed in
\cite{lattuada2016faucet}, this could allow us to safely re-introduce
heuristic elements into the query planner.

We imagine an adaptive operator that can route between a worst-case
optimal and a traditional join-at-a-time dataflow, feeding new inputs
only as either route completes its previous inputs. After running both
plans in parallel for some time, the worse performing dataflow could
be slowly starved and ultimately shut down.

For somewhat predictable data sources, doing so could significantly
reduce the overall dataflow size and complexity, thus supporting more
concurrent users. On data sources that undergo significant changes
over time, doing so would not protect us from sudden catastrophic
skews any longer.

\subsection{Datalog Evaluation Strategies}

At its heart (and mostly thanks to Differential's incremental
computation model), 3DF uses a semi-naive, bottom-up evaluation
strategy. Over the past decades, many more sophisticated Datalog
evaluation strategies have been developed (e.g. \cite{tekle2011more}).

Many of these strategies hinge on finding clause orders, or
introducing additional clauses, s.t. the resulting sequence of clauses
is semantically equivalent to the input clauses, but materialize fewer
results.

More work is needed to understand how strategies such as the magic-set
transform or subsumptive tabling translate into the incremental
dataflow model, and to what extent they overlap with worst-case
optimal join evaluation.

\subsection{Set vs Multiset Semantics} \label{set-vs-multiset}

Over the course of this work we have changed 3DF to expose
Differential's multiset semantics, rather than impose set semantics on
top. The reasons for this are motivated equally by performance
cosiderations as well as user experience concerns. Here we must for
once disagree with \cite{aref2015design}, who state that:

\begin{quote}
  In our experience, [multiset semantics] also makes queries harder
  for the end users to write and to reason about.
\end{quote}

Queries involving grouped aggregations commonly lead to redundant
tuples within each group. Consider for example a simple query summing
the total salary cost per company

\begin{verbatim}
[:find ?company (sum ?salary)
 :where
 [?company :company/employee ?employee]
 [?employee :employee/salary ?salary]]
\end{verbatim}

The implicit grouping and projection expressed by the \texttt{:find}
clause can cause a lot of redundant values, as many employees will
have the same salary. Under set semantics, the resulting sums will not
match user expectations. Epxressed purely in Differential Dataflow,
the results would be ``correct''.

Before this work, 3DF had adopted the \texttt{:with} construct from
the Datomic query language. Re-writing the same query as

\begin{verbatim}
[:find ?company (sum ?salary)
 :with ?employee
 :where
 [?company :company/employee ?employee]
 [?employee :employee/salary ?salary]]
\end{verbatim}

would ensure that two employee salaries equal in value would still be
considered as distinct tuples. Both, imposing set semantics on top of
Differential (via the \texttt{distinct} operator), as well as
implementing the \texttt{:with} construct meant introducing additional
operators into the dataflow. The \texttt{distinct} operator in
particular is stateful and must keep all of its inputs in arranged
form.

Using multiset semantics also allows us to aggregate multiplicites
directly. Differential can operate on multiplicites efficiently and
in-place, whereas operations on data itself must remain immutable (and
thus allocate new tuples). In the above example, 3DF will treat
employee salaries as multiplicites of company eids ($((company,
employee, 12345), t, 1)$ becomes $((company), t, 12345)$). Using
Differential's built-in \texttt{count} operator, these multiplicites
will be consolidated and accumulated in-place, according to any
binary, associative operation under which the multiplicites form a
monoid. Exploiting algebraic structure within the multiplicities to
express more interesting aggregations is a topic of ongoing research
(\cite{abo2016faq}) and will be of great interest to us in the future.

On the input side however, users will expect set semantics, because
that matches the mental modelling of a domain as individual
facts. Transacting a triple such as \texttt{[x :knows y]} twice is
usually expected to simply assert that fact \emph{no matter what was
  true before}, rather than to indicate an increased weight of that
specific edge in a weighted hypergraph.

The Hector operator in fact assumes its inputs to be distinct. For
tuples with multiplicities larger than one, output multiplicites will
not be correct any longer, because they might be multiplied more than
once. This is because prefix and proposal multiplicites are multiplied
\emph{at each extension step}.

Unless otherwise specified by the user, 3DF will therefore apply a
\texttt{distinct} to attribute input streams.

One underexplored aspect of this decision is how it affects the size
of the available plan space. Quoting again \cite{aref2015design}:

\begin{quote}
  [...] traditional SQL-based relational database systems adhere to a
  bag (multiset) semantics, [...]. This design choice drastically
  reduces the opportunity for query optimization, since it often
  happens that queries that are logically equivalent under set
  semantics are not under the bag-semantics.
\end{quote}

We recognize this as a potential trade-off, but haven't yet explored
its effects on our specific setting.

\subsection{Richer Attribute Semantics}

Throughout the discussions in chapter \ref{case-join-ordering} in
particular, we have made no stricter assumptions on attributes than
that they be distinct (as touched on in the previous section). We
suspect that allowing users to express more specific attribute
semantics will open up entirely new optimization avenues.

We have often encountered attributes that should hold at most a single
value for each distinct entity id. For example, an attribute such as
\texttt{:device/speed} might often be intended to hold the most recent
measurement for each device. Assuming an existing fact \texttt{[x
    :device/speed 120]}, users expect a subsequent input \texttt{[x
    :device/speed 200]} to mean both the retraction of the previous
value, as well as the assertion of the current one.

To accomodate this, we have extended 3DF with the concept of
\emph{input semantics}, which can be configured for each attribute
individually. A custom stateful operator enforcing retraction of
previous values is applied to attributes marked
\texttt{CardinalityOne}.

Whenever a CardinalityOne attribute participates in a worst-case
optimal prefix extension of its key symbol to its value symbol, we
know that it will only ever propose zero or one. For star-joins on
exclusively CardinalityOne attributes we can thus guarantee domain
overlap and a uniform distribution of values. While attributes might
still be correlated in adversarial ways, it might still be safe to use
simpler, non worst-case optimal join implementations (delta queries
without an estimation phase, or join-at-a-time pipelines).

Identifying, evaluating, and expressing such special cases within an
extensible framework would be an interesting topic of further
research.

\end{document}
