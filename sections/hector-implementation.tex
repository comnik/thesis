\documentclass[../index.tex]{subfiles}

\begin{document}

For chapters \ref{case-join-state} and \ref{case-join-ordering} we
extended 3DF with a n-way operator based on the Delta-BiGJoin
implementation by \cite{ammar2018distributed}. Delta-BiGJoin itself
relies on the delta query technique, allowing us to evaluate both
delta queries, as well as worst-case optimal join strategies. The
present implementation is based off of \cite{dogsdogsdogs} and
\cite{dataflowjoin}, which are implementations of Delta-BiGJoin for
generalized motif-tracking.

Over the following sections we describe the basic operator
implementation and detail the changes that were required to generalize
it to 3DF's hypergraph data model and indexing structures. We also
added additional implementations of the core interfaces, in order to
support constant bindings, binary predicates, and stratified
negation. We discuss suitable variable orderings, and the use of
logical timestamps to avoid redundant derivation of results across
multiple delta queries. The resulting operator is called \emph{Hector}
and will be referred to as such throughout this chapter.

\subsection{Basic Operation}

At a high-level, the Hector operator provides the following
capability: \emph{Given a set of clauses and a set of target
  variables, find all possible variable assignments that satisfy all
  clauses.} Clauses represent the different kinds of \emph{variable
  bindings}. The clause \texttt{[?e :edge ?v]}, for example,
constrains variables $e$ and $v$ to elements of the \texttt{:edge}
relation, whereas the clause \texttt{[(< ?v 1000)]} restricts the
variable $v$ to values less than 1000.

Some bindings, such as predicates and negation, are exclusively
constraining (shrinking the space of possible values for a given
variable), others, such as attributes and constants, can also provide
values. We will refer to the former as constraints, and to the latter
as bindings from now on.

If no bindings are passed, Hector will throw an error. If only a
single binding is passed, Hector will merely perform a projection onto
the target variables. If only two bindings are passed we have nothing
to gain from the worst-case optimal strategy, because for each of the
source bindings there will only be the one remaining binding left to
propose anything. Similarly, we do not benefit from delta queries
here, because a two-way join would not create any redundant
intermediate arrangements.

For the general-arity case, Hector first employs the delta query
technique. This means that a separate dataflow will be constructed for
each binding that may experience change. By default, all attributes
are assumed to experience change. Within each delta query, we refer
refer to the generating binding as the \emph{source binding}. Review
\autoref{fig:delta-query-plan} for an illustration. The outputs of all
delta queries created for a single Hector stage are concatenated
together. After the final stage, an additional projection onto the
variables requested by the user is performed.

Each delta query is a sequence of the worst-case optimal dataflow join
primitive illustrated in \autoref{fig:wco-illustration}. Each stage
extends changes on the source binding to the next target
variable. Picking an appropriate variable order is discussed in
\autoref{impl-variable-order}. Here Delta-BiGJoin differs somewhat
from non-incremental worst-case optimal join algorithms, in that it
starts off with prefixes binding two variables (the elements of the
source binding), whereas non-incremental algorithms start from the
empty prefix. Dataflow computations must always start with some source
of input. In our case, the finest grained source of input available
are attributes, which already bind two symbols.

In order to get away with this in the general case, we must make sure
to handle conflicts on the variables of each source binding. A
conflict occurs whenever a non-source binding or constraint has
opinions on either of the source variables but would never get a
chance to enforce these opinions. A common example of this is a
constant binding on either of the source variables. The constant
binding will effectively be ignored. Reverse bindings such as
\texttt{[?a :knows ?b] [?b :knows ?a]} cause a similar problem,
because both of their variables are already bound by the other. We
detect conflicts by looking for bindings for which all of their
variables are already bound by the prefix. In our current
implementation, Hector detects all conflicts, but only handles those
with constant bindings. This is done by filtering the source binding
on the constant value.

For the following we will again assume that a suitable variable order
is at hand. We look at the variable order, and the variables bound by
the current prefix and determine from that the next variable $x$, to
which prefixes should be extended. Ignoring the source binding, we
then filter all other bindings down to only those that bind ("talk
about") $x$. From that set of relevant bindings we derive a set of
\emph{prefix extenders}. Each binding type corresponds to a prefix
extender implementation. Our extender implementations are detailed in
\autoref{impl-extenders}. Extenders participate in the three-step
process illustrated in \autoref{fig:wco-illustration}:

\begin{enumerate}
  \item \textbf{Count} In the count step we determine for each source
    prefix the extender that will propose the fewest extensions for
    it. This is done by passing a stream of (prefix, count,
    extender)-triples through each extender's \texttt{count}
    operator. Initially all counts are set to infinity. The count step
    results in a stream of \emph{nominations} for each extender
    (prefixes for which the respective extender has declared the
    fewest extensions).

  \item \textbf{Propose} Nomination streams are passed through their
    respective extender's \texttt{propose} operator. Propose
    implementations materialize the extensions they counted in the
    previous step and output a stream of \texttt{(prefix extension)}
    pairs.

  \item \textbf{Validate} Each extender's proposals are validated by
    all other extenders. This is done by passing the stream of
    proposed pairs through each extender's \texttt{validate}
    operator. Intuitively, validate implementations compute the
    intersection of the set of proposals with extensions that they
    themselves would have proposed, had they been nominated to extend.
\end{enumerate}

Assuming that the count, propose, and validate implementations for
prefix extenders satisfy certain conditions, each extensions will only
do work on the order of the worst-case result set bound. With the
basic operation of the Hector operator covered, we now describe our
prefix extender implementations and how they satisfy the requirements
for worst-case optimality.

\subsection{Prefix Extenders} \label{impl-extenders}

From the discussion in \cite{veldhuizen2012leapfrog} we learn that the
Leapfrog Triejoin worst-case optimal join algorithm can be utilized to
implement many common features of relational query engines:
conjunctions (joins), disjunctions (unions), negations (antijoins),
and filtering by various predicates. This is achieved by additional
implementations of the core trie iterator interface. Similar
extensions exist for Delta-BiGJoin.

Prefix extenders must provide \texttt{count}, \texttt{propose}, and
\texttt{validate} operators. While conceptually simple, these
implementations must not break the worst-case optimality, meaning that
they can only do work proportional to the lowest number of extensions
proposed by any of them (for a given extension stage).

The count operator must therefore avoid materializing and counting all
proposals, as doing so for every extender would immediately violate
the worst-case optimality. Implementations like Leapfrog TrieJoin and
our own will accept count implementations that take logarithmic time
(e.g. looking up prefixes in a suitable index). As long as only the
winning extender is asked to propose, the propose implementation must
only make sure to propose tuples with constant delay. A naive
validation implementation might materialize all of its proposals and
compute the set intersection. This of course violates worst-case
optimality. However we may again take logarithmic time to lookup
proposed extensions in an index.

\textbf{CollectionExtender} \cite{dogsdogsdogs} gives an
implementation of a prefix extender for Differential collections of
key-value pairs, which we have largely adapted to implement prefix
extension for 3DF attributes. The resulting implementation maintains
attributes across three separate indices, corresponding to the count,
propose, and validate operations. The count index maintains the number
of distinct values for each key, the propose index maps keys to
values, and the validate index maintains key-value pairs. Each index
is implemented as a Differential arrangement, which support lookups in
logarithmic time.

\textbf{ConstantExtender} Prefix extension for constant bindings is
not backed by data other than the constant value. The count
implementation simply issues a count of one for each prefix. Propose
proposes for each prefix the constant value. Finally validate filters
the stream of proposed extensions to only include those which match
the constant value. These operations trivially satisfy the complexity
bounds.

\textbf{BinaryPredicateExtender} Prefix extension for binary predicate
bindings is again not backed by data. Predicate extenders can never be
asked to propose, because they would have to propose infinitely many
extensions. The count implementation is therefore a noop, whereas the
propose implementation causes a runtime error, should the predicate
extender receive a nomination. The validate implementation will apply
the desired predicate to each proposed extension and to the prefix
offset binding the second argument. The supported binary predicates
can be evaluated in constant time.

\textbf{AntijoinExtender} Antijoin extenders implement negation as set
difference and thus wrap any of the other extenders. Due to the
stratification requirement, antijoin extenders must not be asked to
propose, and thus implement count as a noop and propose as a runtime
error. During validation, antijoin extenders will subtract all
extensions that the wrapped extender has validated from the result
stream. Therefore, as long as the wrapped extender satisfies
worst-case optimality, the antijoin extender will too.

These new indexing requirements had to be integrated into
3DF. Throughout this work we rely on base relations that are
maintained in an indexed form and that can be shared across all
dataflows. As described in this section, worst-case optimal prefix
extenders require multiple different indices, in order to meet their
complexity bounds. Additionally, whenever attributes are placed at a
stage in a delta query, either of their symbols might be bound by the
prefixes at that point. Therefore, reversed versions of all extender
indices must be maintained as well.

We introduced a \texttt{CollectionIndex} structure which holds all
three arrangements for a given direction and a given attribute. 3DF
workers maintain separate mappings from attribute names to their
forward and reverse collection indices. This simplifies the types and
ownership involved. Collection indices implement a subset of
Differential's arrangement API: an \texttt{import} method for
importing all internal traces into a top-level scope, as well as an
\texttt{enter\_at} method to bring all imported internal arrangements
into a nested scope with an additional timestamp coordinate.

Two additional implementation details become relvant in practice:
picking a suitable variable order and preventing redundant outputs
during concurrent execution of delta queries. We will discuss these in
the following two sections.

\subsection{Suitable Variable Orderings} \label{impl-variable-order}

We have deferred the problem of choosing a suitable variable
ordering. While different orderings do not affect the theoretical
guarantees, they can make a significant difference in practice. A
suitable ordering is one that includes all variables bound by any of
the participating bindings and which ensures that for each variable
$v$ there exists at least one binding that binds $v$. Additionally, we
require that at least one of the (non-unary) bindings on $v$ has its
other side already bound by the prefixes at that point. This is
analogous to avoid two-way joins on disjoint relations. While this can
be limiting in edge cases, it simplifies the implementation and helps
avoid pathological orderings in practice. Given the following clauses:

\begin{lstlisting}[language=datalog, style=colorlog]
[?a :knows ?b]
[?b :knows ?c]
[?c :knows ?d]
[?a :knows ?d]
\end{lstlisting}

\texttt{[?a ?b ?c ?d ?e]} would be a suitable order, whereas
\texttt{[?a ?c ?b ?d]} would'nt, because none of the bindings can
extend the prefix \texttt{[?a]} to \texttt{?c}.

However starting with the prefix \texttt{[?a ?b]}, it would be equally
valid to extend first to \texttt{?c} and then to \texttt{?d} or vice
versa. Deciding between the two is the problem of finding an
\emph{optimal} variable ordering. This question is significantly more
involved and beyond the scope of this work. It is treated extensively
in \cite{abo2016faq}.

\subsection{Lexicographic Timestamps} \label{impl-altneu}

All delta pipelines are executed concurrently at the dataflow
level. This can lead to inconsistencies as we might derive the same
output change on multiple pipelines, when sources change
concurrently. Assume for example we have a graph, initially containing
only a single edge $(100,300)$, and two delta pipelines:

\begin{verbatim}
(I) d(a,b) -> (b,c) -> (a,c)
(II) d(b,c) -> (a,b) -> (a,c)
\end{verbatim}

At some logical time $t$ we then input two new edges
$((100,200),t,+1)$ and $((200,300),t,+1)$. Crucially, both inputs are
then logically available in all relations at time $t$. Along pipeline
(I), the input edge $(100,200)$ will therefore match the indexed edge
$(200,300)$ and thus cause the derivation of a single output change
$((100,200,300),t,+1)$. Correspondingly, along pipeline (II) the input
edge $(200,300)$ will match the indexed edge $(100,200)$, leading to a
now redundant output change $((100,200,300),t,+1)$.

To prevent this, we must impose an arbitrary logical order on the
computation. In particular, we must ensure that input changes are not
logically available at all relations simultaneously (even though
physically, they might already be indexed). For this we fix an
arbitrary sequential order, e.g. $(a,b),(b,c),(a,c)$.

Delta pipelines are therefore created within a new, nested scope using
a modified, lexicographic timestamp type. Timestamps $t$ along each
pipeline are extended to $(t,a \in {0,1})$, where $a$ is a boolean
indicating the relative position of the source binding and the
extending relation. These extended timestamps are ordered
lexicographically, ensuring in particular that $(t,0) <
(t,1)$. Attribute arrangements are imported and wrapped with their
corresponding timestamps. We cache imported arrangements to prevent
redundant imports.

Revisiting the scenario from above, the input edges would now be
logically available in relation $(b,c)$ on pipeline (II), but not on
pipeline (I), because $(b,c)$ appears after $(a,b)$ in the relation
order. Accordingly, only pipeline (II) would now derive the correct
output change. This concludes our discussion of all relevant details
of the Hector operator.

\end{document}
