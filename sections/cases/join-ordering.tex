\documentclass[../catalog.tex]{subfiles}

\begin{document}

\subsection{Problem}

As has been widely documented within the database community
(\cite{leis2015good}, \cite{lohman2014query}), many complex queries
are highly sensitive to join ordering. We have discussed the
established heuristical approaches originating from
\cite{selinger1979access} in \autoref{known-techniques} and hinted at
their various shortcomings.

There we concluded that in practice, cardinality estimation under the
traditional assumptions (uniformity, uncorrelated join predicates,
overlapping key domains, and full access to meaningful statistics) can
not avoid disastrous plans.

While improvements in probabilistic modeling of correlations, on-line
gathering of index statistics, and adaptive approaches to query
execution such as Eddies provide many interesting avenues of inquiry,
we are primarily interested in approaches that make disastrous plans
impossible — possibly at the expense of best- and average-case
performance.

The reasons for this are two-fold: first, and most importantly,
disastrous query plans violate all three of the desired properties
outlined in \ref{problem}. Second, from experiences with 3DF itself,
and other declarative systems such as the Prolog language, we know
that the mere \emph{possibility} of unpredictable, severe performance
degradations break the fundamental promise of the declarative
abstraction, and force users to reason defensively about the system
runtime.

We can further distinguish between different classes of problematic
queries:

\begin{itemize}

  \item \textbf{Worst-Case} Queries for which any join-at-a-time plan
    will process asymptotically more tuples than could possibly be
    contained in the result set. A class of queries for which this is
    case is formalized in \cite{ngo2012worst}. Intuitively, these are
    queries that compute the $d$-way join of all $(d-1)$-ary
    projections of the result set. An instance of this query class is
    the triangle query, which computes $(a,b,c)$ from its projections
    $(a,b)$, $(b,c)$, and $(a,c)$.

    Triangles and cyclic sub-pattern queries in general have many
    applications. The LDBC Social Network Benchmark query
    \texttt{BI/read/11}, for example, contains a triangle between
    comments, messages, and tags, in order to find unrelated
    replies. Query \texttt{BI/read/19} asks for a 4-clique between
    persons, comments, and messages, in order to determine for each
    person the set of strangers they have interacted with.

  \item \textbf{Average-Case} Queries across correlated columns,
    i.e. on data where the assumptions of uniformity, independence,
    and overlapping domains are violated to varying extents. For
    queries in this class, accurate estimation of cardinalties can
    make a huge difference for join-at-a-time plans. These are queries
    whose hypergraph representation is cycle-free.

    LDBC query \texttt{BI/read/2}, for example, joins countries,
    cities, persons, messages, and tags. The order of this traversal
    can make a significant difference, depending on the relative
    selectivity, and additional predicates applied to individual
    relations (e.g. ``consider only cities in germany'').

  \item \textbf{Best-Case} Queries across uncorrelated, uniformly
    distributed columns. Their hypergraph representation is
    cycle-free, and the assumptions of traditional cardinality
    estimation hold. These are best-case queries.

    Examples would be star-joins on uncorrelated columns, where each
    column holds at most a fixed number of values for each
    key. E.g. joining user ids exclusively on single-cardinality
    attributes such as username, email, or age.
    
\end{itemize}

\subsection{Remedy}

We covered recent advances in the area of worst-case optimal
(\emph{wco}) join processing that describe n-way join algorithms with
improved asymptotic complexity for queries in the worst-case
class. These algorithms distinguish themselves from traditional
approaches, by checking cardinalities for individual tuples. They can
thus ensure that any extension of a set of prefixes to a new variable
will look at the smallest number of tuples, even in the face of
correlated or non-uniformly distributed relations.

For many average- and best-case queries however, sub-optimal join
orderings have an adverse effect on performance that, although not
asymptotically worse, is significant in practice. In these cases, the
variable order chosen by a wco join algorithm matters much more.

For example, given prefixes \texttt{[?a ?b]} and relations \texttt{[?a
    :r1 ?c], [?a :r2 ?d]}, we might choose to first extend to
\texttt{?c} via \texttt{:r1}, or instead to \texttt{?d} via
\texttt{:r2}. Choosing one over the other has no bearing on the
asymptotic result set bound. Still, for non-uniformly distributed
relations, some prefixes might produce many more matches in
\texttt{:r1}, whereas for others it would be the other way
around. Picking the optimal variable order is thus again a
data-dependent cardinality estimation problem, if one with a better
safety net.

Further, wco algorithms perform significantly more work per tuple,
than traditional two-way joins. For example, at each extension stage,
each tuple requires multiple index lookups in order to determine the
optimal extension strategy.

We thus expect a wco approach to significantly improve robustness on
queries in the worst-case class, but to adversly affect performance on
average- and best-case queries, as compared to traditional,
heuristics-based approaches.

Our particular implementation of a worst-case optimal dataflow join
operator is detailed in \autoref{impl-hector}.

\subsection{Evaluation}

First we want to examine whether a worst-case optimal join strategy
does lead to more predictable latencies on cyclic queries,
\emph{irrespective of the clause order provided by the user}. For this
we will consider the triangle query \texttt{[?a :edge ?b] [?b :edge
    ?c] [?a :edge ?c]} on the livejournal graph. The graph contains 68
million edges. We introduce edges in node order, feeding them into a
single attribute. The i-th round of input is therefore that in which
all edges starting at node i are ingested. We use a single worker,
running on a single core of a 2.7 GHz Intel Core i5, with 16GB RAM
available.

We run this setup for both strategies — consecutive binary joins
(\emph{JoinJoinJoin}) and worst-case optimal (\emph{Hector} — and for
three different input clause orderings each, measuring completion time
for each round of inputs. The resulting latency distributions are
shown in figure \ref{fig:triangle-cdfs}.

\begin{figure}[h!]
  \includegraphics[width=1.0\linewidth]{results/triangles/out/all_cdfs}
  \caption{Triangle Query - Latency CCDFs}
  \label{fig:triangle-cdfs}
\end{figure}

As expected, we observe that performance of the JoinJoinJoin strategy
is highly sensitive to clause order. In particular, the first ordering
(\texttt{([a b] [a c]) [b c]}), runs out of memory at node 87, while
the third ordering (\texttt{([a c] [b c]) [a b]}) consistently
outperforms all of the tested strategies.

The worst-case optimal strategy performs consistently well for all
three input clause orderings and consistently outperforms the two
disastrous join orderings by one to many orders of magnitude. The best
performing JoinJoinJoin order can outperform Hector, but with a
similar long-tail and twice the maximum latency.

These observations remain valid when measuring thrice the number of
rounds (\autoref{fig:triangle-cdfs-extended}), as well as when
increasing the batch size to introduce one thousand nodes at a time
(\autoref{fig:triangle-cdfs-batch}).

\begin{figure}[h!]
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=1.0\linewidth]{results/triangles/out/extended_cdf}
    \caption{More Rounds}
    \label{fig:triangle-cdfs-extended}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=1.0\linewidth]{results/triangles/out/batch_cdf}
    \caption{Batch Size 1000}
    \label{fig:triangle-cdfs-batch}
  \end{subfigure}
\end{figure}

Next we evaluate Hector on the \texttt{BI/read/2} query from the LDBC
dataset. We used the LDBC data generator to generate a dataset
simulating the activity of 10,000 users in a social network. We
registered each query plan and then ingested the data in small
batches, measuring the completion time for each input
epoch. \autoref{fig:average-cdfs} shows the resulting latency
distributions for two different join-at-a-time orders and Hector.

\begin{figure}[h!]
  \includegraphics[width=1.0\linewidth]{results/bi_read_2/out/all_cdfs}
  \caption{BI/read/2 - Latency CCDFs}
  \label{fig:average-cdfs}
\end{figure}

Finally we evaluated a simple star-join of posts across four
one-to-one attributes (\texttt{:post/ip, :post/browser,
  :post/language, :post/content}). Each post in the dataset has
exactly one value assigned for each of these
attributes. \autoref{fig:best-case-cdfs} shows the resulting latency
distributions.

\begin{figure}[h!]
  \includegraphics[width=1.0\linewidth]{results/best_case/out/all_cdfs}
  \caption{Best-Case Star-Join - Latency CCDFs}
  \label{fig:best-case-cdfs}
\end{figure}

\end{document}
